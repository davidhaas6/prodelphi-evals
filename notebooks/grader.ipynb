{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Prodelphi's Virtual Customers: Do They Stay in Character?  \n",
    "\n",
    "AI-generated interviews can provide valuable insights, but their reliability depends on the consistency and authenticity of the simulated personas. Without a gold-standard dataset of <demographics, interview> pairs, we turn to a rubric-based LLM grader to systematically assess fidelity.  \n",
    "\n",
    "This grader evaluates whether Prodelphi’s virtual customers remain true to their assigned personas by analyzing:  \n",
    "- **Character Consistency**: Does the interviewee’s personality, background, and motivations remain stable throughout?  \n",
    "- **Domain Expertise**: Does the character demonstrate knowledge (or lack thereof) appropriate to their profile?  \n",
    "- **Tone and Authenticity**: Is the character's speech pattern, reasoning, and emotional expression coherent and realistic?  \n",
    "- **Guideline Adherence**: Does the interview align with predefined customer archetypes and behavioral expectations?  \n",
    "\n",
    "By rigorously evaluating these aspects, we ensure that Prodelphi’s AI-driven interviews remain credible, structured, and aligned with the personas they represent.  \n",
    "\n",
    "\n",
    "See `src/baml_src/grader.baml` for the prompt and rubric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Set up environment\n",
    "import sys\n",
    "import os\n",
    "import dotenv\n",
    "sys.path.append(os.path.abspath(\"../src\"))  # workaround for loading baml lib\n",
    "dotenv.load_dotenv('../.env')\n",
    "\n",
    "# Functional needs\n",
    "import json\n",
    "from datetime import datetime\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "\n",
    "import baml_client.async_client as client \n",
    "\n",
    "def timestamp():\n",
    "    return datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = '../data/clean_022625.json' # Pairs of interview transcripts and subject info\n",
    "with open(DATASET_PATH) as f:\n",
    "    study_list = json.load(f)\n",
    "\n",
    "print(f\"{len(study_list)} studies in dataset\")\n",
    "print(f\"Study keys: {study_list[0].keys()}\\nInterview keys: {study_list[0]['interviews'][0].keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_character(archetype, backstory) -> str:\n",
    "    new_arch = {**archetype}\n",
    "    del new_arch['emoji']\n",
    "    new_arch['backstory'] = backstory\n",
    "    return json.dumps(new_arch, indent=2)\n",
    "\n",
    "def format_interview(messages, num=None) -> str:\n",
    "    if num:\n",
    "        interview_content = f\"# Interview {num}:\\n\"\n",
    "    else:\n",
    "        interview_content = \"# Interview:\\n\"\n",
    "    interview_content += \"\\n\".join(\n",
    "        [f\"Q: {msg['content']}\" if msg[\"role\"] == \"user\" else f\"A: {msg['content']}\" for msg in messages]\n",
    "    )\n",
    "    return interview_content\n",
    "\n",
    "# sample = study_list[0]['interviews'][0]\n",
    "# test_char = format_character(sample['customer_archetype'], sample['backstory'])\n",
    "# test_intvw = format_interview(sample['transcript'])\n",
    "# out = client.b.EvalAdherence(test_char,test_intvw)\n",
    "# out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate interviews against rubric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def eval_adherence_async(client, test_char, test_intvw, study_id, character_name):\n",
    "    \"\"\"Execute the rubric evaluation and return a structured result\"\"\"\n",
    "    data = await client.b.EvalAdherence(test_char, test_intvw)\n",
    "    return {**data.model_dump(), 'study_id': study_id, 'character_name': character_name}\n",
    "\n",
    "async def main_adherence_eval():\n",
    "    tasks = [\n",
    "        eval_adherence_async(\n",
    "            client,\n",
    "            format_character(interview['customer_archetype'], interview['backstory']),\n",
    "            format_interview(interview['transcript']),\n",
    "            study.get('study_id', 'null'),\n",
    "            interview['customer_archetype']['customer_name']\n",
    "        )\n",
    "        for study in study_list\n",
    "        for interview in study.get('interviews', [])\n",
    "    ]\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n",
    "evals = await main_adherence_eval()\n",
    "\n",
    "df = pd.DataFrame(evals)\n",
    "df.to_csv(f'../results/adherence_{timestamp()}.csv', index=False)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the evaluator\n",
    "\n",
    "Is our LLM grader useful and accurate? How does it react if we intentionally misalign the peronsas and interviews?\n",
    "\n",
    "### Experiments\n",
    " - Shuffle characters across projects\n",
    " - All personalities -- same interview. how does grader respond?\n",
    " - One personality -- different interviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def rando_intra_study_adherence_eval():\n",
    "    all_evals = []\n",
    "    tasks = []\n",
    "\n",
    "    for study in study_list:\n",
    "        study_id = study.get('study_id', 'null')\n",
    "        intaviews = study.get('interviews', [])\n",
    "        for i, interview in enumerate(intaviews):\n",
    "            intvw_id = (i+1) % len(intaviews)\n",
    "            character_name = interview['customer_archetype']['customer_name']\n",
    "            character_info = format_character(interview['customer_archetype'], interview['backstory'])\n",
    "            transcript = format_interview(intaviews[intvw_id]['transcript'])\n",
    "            task = eval_adherence_async(client, character_info, transcript, study_id, character_name)\n",
    "            tasks.append(task)\n",
    "    \n",
    "    all_evals = await asyncio.gather(*tasks)\n",
    "    \n",
    "    # Do something with all_evals\n",
    "    print(all_evals)\n",
    "    return all_evals\n",
    "\n",
    "rando_intra_study_evals = await rando_intra_study_adherence_eval()\n",
    "rando_df = pd.DataFrame(rando_intra_study_evals)\n",
    "rando_df.to_csv(f'../results/adherence_rando_{timestamp()}.csv', index=False)\n",
    "rando_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
